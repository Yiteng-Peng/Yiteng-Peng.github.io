---
date: 2024-10-19 10:20:07
permalink: /pages/3bf3f25340d2
tags: 
  - adversarial
---

学习：<https://nicholas.carlini.com/writing/2018/adversarial-machine-learning-reading-list.html> 所提及的基础论文。

### Foundation

1. Evasion attacks against machine learning at test time. 2013

这篇2013年的文章据说是第一个提出Adversarial Attack这个概念的，对这个问题定义得很清楚：

- 对于一个x，找到x'，使得降低损失函数g(x')的值尽可能小，进而使得分类器f(x)预测相反。
- x'可能是有限制的（例如输入是邮件文本时，你能改变的是文本而不能是像素）且希望x和x'的距离较小。
- 这里着重提出，损失函数在0附近时，似乎就不是很有价值（outside of the samples’ support）。

对于最后一点这个challenge，本文在正常的损失函数g(x)以外引入了一项惩罚项来计算x和其他同类x的距离，以保证x处于“densely populated regions”之中。

2. Intriguing properties of neural networks. 2014

首先讨论了图片的语义信息是由于单个神经元记忆的还是由于其他原因，否定了之前的每个神经元会表示某个特定的语义这一观点。（可能是当年比较流行的讨论？）

然后通过优化下式来得到对抗样本：

$$
min \quad c|r| + loss_f(x+r, l)
$$

也就是核心还是在原样本附近找到一个足够接近但是损失函数却异常很大的样本。然后论文中还进一步做了关于transferability的讨论。

3. Explaining and Harnessing Adversarial Examples. 2015

TODO

可以看到，这些论文或许提出的公式很简单，但是介绍的原理意义却非常的详尽清晰，因此一个工作的价值并不在于提出的技术是否足够复杂，而在于背后的insight，意义和对于这一现象的分析是否足够深入。