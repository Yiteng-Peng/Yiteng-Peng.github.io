---
date: 2024-10-19 10:20:07
permalink: /pages/3bf3f25340d2
tags: 
  - adversarial
  - ai security
---

学习：<https://nicholas.carlini.com/writing/2018/adversarial-machine-learning-reading-list.html> 所提及的基础论文。

### Foundation

1. Evasion attacks against machine learning at test time. 2013

这篇2013年的文章据说是第一个提出Adversarial Attack这个概念的，对这个问题定义得很清楚：

- 对于一个x，找到x'，使得降低损失函数g(x')的值尽可能小，进而使得分类器f(x)预测相反。
- x'可能是有限制的（例如输入是邮件文本时，你能改变的是文本而不能是像素）且希望x和x'的距离较小。
- 这里着重提出，损失函数在0附近时，似乎就不是很有价值（outside of the samples’ support）。

对于最后一点这个challenge，本文在正常的损失函数g(x)以外引入了一项惩罚项来计算x和其他同类x的距离，以保证x处于“densely populated regions”之中。

2. Intriguing properties of neural networks. 2014

首先讨论了图片的语义信息是由于单个神经元记忆的还是由于其他原因，否定了之前的每个神经元会表示某个特定的语义这一观点。（可能是当年比较流行的讨论？）

然后通过优化下式来得到对抗样本：

$$
min \quad c|r| + loss_f(x+r, l)
$$

也就是核心还是在原样本附近找到一个足够接近但是损失函数却异常很大的样本。然后论文中还进一步做了关于transferability的讨论。

3. Explaining and Harnessing Adversarial Examples. 2015

讨论了对抗样本并非是因为非线性导致的，线性模型上也同样可能会产生对抗样本。同时给出了一种计算对抗样本的方式：FGSM，并基于此讨论相对于具体的离散的点，对抗样本的方向是更重要的。同时还讨论了关于对抗训练部分的内容。

通过这三篇论文可以对当年对抗样本早期起步的研究有一个基础的了解，也能让人认知到一个扎实的，深入的研究应该怎么进行。

可以看到，这些论文或许提出的公式很简单，但是介绍的原理意义却非常的详尽清晰，因此一个工作的价值并不在于提出的技术是否足够复杂，而在于背后的insight，意义和对于这一现象的分析是否足够深入。

### Transferability

1. Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples. 2016

2. Delving into Transferable Adversarial Examples and Black-box Attacks. 2017

3. Universal adversarial perturbations. 2017

老实说，它的算法不是那么的令人surprising，但是每次读到这篇，都会被它出乎意料的结论和现象震撼，这应该是UAP的第一篇，虽然有点反直觉，但是UAP它确确实实的真实存在。

### Physical-World Attacks

1. 